# layers
3 5 2 
# activation functions (0=Linear, 1=Sigmoid, 2=ReLU, 3=Tanh, 4=Softmax)
1 0 
# weights
0.47885 0.897433 0.700238 
0.271321 0.804914 0.75141 
0.101633 0.697392 0.144179 
0.20334 0.204955 0.0650653 
0.220385 0.228439 0.646953 
0.435727 0.606756 -0.0535573 0.382353 0.141568 
0.0211301 -0.0718241 0.458108 0.296374 0.23014 
# biases
0.097886 0.202725 0.195579 0.927591 0.262946 
-0.167411 0.269795 
